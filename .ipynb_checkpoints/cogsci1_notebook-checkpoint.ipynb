{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Cog Sci 1] 7 Plus-or-Minus 2 Billion: Human Brains vs. Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Professor Paul Li "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Estimated Time: 50 Minutes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a href=\"https://clalliance.org/blog/computing-brains-neuroscience-machine-intelligence-and-big-data-in-the-cognitive-classroom/\"><img src=\"images/brain-data-image.jpg\"></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics Covered \n",
    "Welcome! This lab will be an introduction to Big Data and the Human Brain as well as a gentle introduction to Jupyter Notebooks. By the end of this lab you will be able to: \n",
    "1. Define _Big Data_ and _dimensions_ in reference to a dataset\n",
    "2. Explain the high-level steps of simple computer facial recognition\n",
    "3. Identify reasons related to human cognition limits that we reduce the dimensions of big data during analysis\n",
    "4. Identify human ethics limitations/consequences that can result from reducing the dimensions of a large dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents \n",
    "\n",
    "1. [Jupyter Notebooks](#1)\n",
    "   - [Running Cells](#1.1)\n",
    "   - [The Data](#1.2)\n",
    "   - [Context](#1.3)\n",
    "<br/><br/>\n",
    "2. [Big Data](#2) \n",
    "   - [What is Big Data?](#2.1) \n",
    "   - [Dimensionality](#2.2)\n",
    "<br/><br/>\n",
    "3. [Computer Vision: Recognizing Faces In An Image](#3)\n",
    "   - [Overview of Facial Recognition](#3.1)\n",
    "   - [Ethics of Facial Recognition](#3.2) \n",
    "   - [How Do These Systems Work?](#3.3)\n",
    "   - [How Do Humans Recognize Faces?](#3.4)\n",
    "<br/><br/>\n",
    "4. [Dimensionality Reduction](#4)\n",
    "   - [Discussion Question](#4.1)\n",
    "<br/><br/>\n",
    "5. [Big Data Can Be Too Big!](#5)\n",
    "   - [Why Would We Simplify Data? ](#5.1)\n",
    "   - [What Are The Risks With Simplifying Data?](#5.2)\n",
    "<br/><br/>\n",
    "6. [Conclusion](#6)\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a href=\"http://data8.org/connector/Cognitive%20Science/\"><img src=\"images/cogsci88.png\" width=700 height=150></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Jupyter Notebooks <a id='1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Jupyter Notebook is an online, interactive computing environment, composed of different types of __cells__. Cells are chunks of code or text that are used to break up a larger notebook into smaller, more manageable parts and to let the viewer modify and interact with the elements of the notebook.\n",
    "\n",
    "Notice that the notebook consists of 2 different kinds of cells: **markdown** and **code**. A markdown cell (like this one) contains text, while a code cell contains expressions in Python, the programming language in this Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Running Cells <a id='1.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Running\" a cell is similar to pressing 'Enter' on a calculator once you've typed in an expression; it computes all of the expressions contained within the cell.\n",
    "\n",
    "To run a code cell, you can do one of the following:\n",
    "- press __Shift + Enter__\n",
    "- click __Cell -> Run Cells__ in the toolbar at the top of the screen.\n",
    "\n",
    "You can navigate the cells by either clicking on them or by using your up and down arrow keys. Try running the cell below to see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of the cell consists of the text/code that is contained within the cell's enclosing box. Here, the input is an expression in Python that \"prints\" or repeats whatever text or number is passed in. \n",
    "\n",
    "The output of running a cell is shown in the line immediately after it. Notice that markdown cells have no output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The Data <a id='1.2'></a>\n",
    "\n",
    "In this lab we will be using the Olivetti faces dataset that contains a set of face images that were captured between April 1992 and April 1994 at AT&T Laboratories Cambridge. There are 10 different images of 40 distinct subjects. The images were not taken in the same lighting conditions or time of day, and the subjects did not all have the same facial expressions. However, all of the images were taken against a dark background with the subjects facing the camera. It is important to note that no efforts were made to create a diversified, unbiased population sample, and the participants were not chosen randomly. The images in this dataset do not represent the wider population of the US at the time, so any algorithm that was fed this data cannot be extrapolated to the rest of the country. Some example pictures are in the cell below. Check out this [link](https://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html) for more information!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a href=\"http://terminalcoders.blogspot.com/2017/03/at-face-database-in-png.html\"><img src=\"images/olivetti_faces.png\" width=450 height=450></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Context <a id='1.3'></a>\n",
    "\n",
    "This lab explores how cognitive limits encourage different techniques to simplifying information, such as computer programs breaking down data into different parts. This is similar to how the nervous system breaks down information that it receives from the environment. Our brains and big data algorithms both need to simplify the mass amounts of data they receive in order to efficiently process it and output a result. You will learn about simple facial recognition and how this relates to the brain processing images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Big Data <a id='2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a href=\"https://pixabay.com/images/search/computer%20science/\"><img src=\"images/big_data.png\" width=800 height=200></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 What is Big Data? <a id='2.1'></a>\n",
    "The term _Big Data_ seems to be a buzz word everywhere lately, but what really is big data? Big data refers to an extremely large dataset that may be analyzed to reveal patterns, trends, or associations, especially relating to human behavior. Many companies and organizations collect data on every transaction or interaction of each user or consumer, so the data can grow very quickly! \n",
    "\n",
    "Consider the image data that your brain deals with. We are constantly processing information from our eyes, which grows with every passing second. This adds up to a lot of data to process. While the dataset we will use in this notebook is not that big, it is still important to recognize that big data is everywhere. Both computer programs and our brains deal with large amounts of data every day and have developed strategies of efficiently processing this information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dimensionality <a id='2.2'></a>\n",
    "\n",
    "How is the size of a dataset measured? Dimensionality refers to how many features or variables a dataset has. For example, if Berkeley had a dataset of students with each row, or observation, representing a student, some variables might be: year, major, residency, units, gpa, etc. A high dimensional dataset means that the number of features is very large and can exceed the number of observations, and calculations can become difficult. Later in this lab, you will learn about how reducing the dimensionality of your data can affect an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Computer Vision: Recognizing Faces In An Image <a id='3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Overview of Facial Recognition <a id='3.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facial recognition systems are computer programs that analyze images of human faces in order to identify the individuals present in them. These systems can be used for general surveillance in a public setting with public video cameras, for personal security purposes, and in many other situations. A well known example of facial recognition is the Face ID feature of the iPhone X. The phone unlocks when you enter a password, or alternatively if it recognizes your face. In general, these systems work by comparing selected facial features from a given image to faces that are already known within a database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a href=\"https://www.nytimes.com/2019/05/15/business/facial-recognition-software-controversy.html\"><img src=\"images/facial_recognition.png\" width=500 height=400></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Ethics of Facial Recognition <a id='3.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to recognize that these systems are not correct and the mistakes they make can be devastating to certain groups. Two well publicized controversies include Hewlett-Packard's face-tracking web camera and Google's facial recognition software for their Photos application. Desi Cryer, an African American man, uploaded a [video](https://www.youtube.com/watch?v=t4DT3tQqgRM) in 2009 to YouTube showing that the face-tracking web camera did not track his face, while it had no problems following his white colleague. In a separate instance, Google's Photos application labeled some African American people as \"gorillas.\" You can read more about the Google incident [here](https://www.cnn.com/2015/07/02/tech/google-image-recognition-gorillas-tag/index.html).\n",
    "\n",
    "Part of the problem with these facial recognition systems is that the data they use to train their algorithms is flawed. A common dataset used to train these systems is more than 75% male and more than 80% white, which could result in an algorithm classifying minorities in this dataset incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a href=\"https://pixabay.com/illustrations/flat-recognition-facial-face-woman-3252983/\"><img src=\"images/facial_rec_new.png\" width=400 height=200></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 How Do These Systems Work? <a id='3.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facial recognition systems use computer algorithms to pick out distinctive features and details of a person's face. The computer looks for ways that faces can be different, such as the distance between eyes, shading on the face, and many other features. Below are the steps of how common algorithms work:\n",
    "\n",
    "1. Image is captured\n",
    "2. Eye locations are determined\n",
    "3. Image is converted to grayscale and cropped\n",
    "4. Image is converted to a template used by the program for facial comparison results\n",
    "5. Image is searched and matched using an algorithm to compare the template to other templates on file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a href=\"https://www.eff.org/pages/face-recognition\"><img src=\"images/facial_rec_steps.png\" width=350 height=150></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 How Do Humans Recognize Faces? <a id='3.4'></a>\n",
    "\n",
    "Our ability to recognize faces quickly and efficiently is remarkable. But how do we actually do it? Your brain can identify items and faces within milliseconds through a complex process.\n",
    "\n",
    "In early face recognition processing, the occipital lobe recognizes individual features of a face, such as the eyes, ears, nose, etc. Then, the fusiform gyrus, an area of the brain that is involved when you look at a face, is activated. It is responsible for holistic information, meaning it puts all of the information from the occipital lobe together for further processing. Then, this whole face is compared to previous faces that you've seen, and the determination of recognition is completed. Humans recognize faces as a \"sum of parts,\" meaning that they recognize the face as a whole as opposed to recognizing individual features. In contrast, a computer recognizes the individual features of a face, and then if enough of these features are matched in its database, it determines the corresponding face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dimensionality Reduction <a id='4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, we need to run the following cell to import packages that will be used later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Import packages that will be used later\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the data into the notebook. Again, you don't have to know how the code is implemented, but think of it as \"opening\" the data like you would open an Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Run to load the data\n",
    "faces = fetch_olivetti_faces()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a dataset of facial images with a high number of dimensions, it can be very taxing for a computer to work with this data. Often, a process used to counter this issue is dimensionality reduction. When working with big data, we can perform an analysis where we look at the dimensions, or components, and order them in terms of their significance to the structure of the dataset. We can then reduce the number of dimensions by removing the least important components.\n",
    "\n",
    "To see this in action, we can perform dimensionality reduction on the Olivetti faces dataset that we loaded. Each image is of size 64 x 64 pixels, and as a result the dimensionality is over 4,000. However, through this analysis we can easily reduce to 400 components and get approximations that are practically indistinguishable from the original images, and we can reduce by even more and still get close approximations.\n",
    "\n",
    "Run the following cell below. With the widget, try changing the number of components using the slider bar to see how the accuracy changes. Look for when the faces become unrecognizable with their original images. To start, try to see what happens when we choose 50 components, 10 components and just 1 component. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04867c5d468a4ce6815d13a069169770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=200, continuous_update=False, description='Number of Components:', layou…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dimensionality_reduction(x):\n",
    "    pca = decomposition.PCA(n_components=x)\n",
    "    pca.fit(faces.data)\n",
    "    components = pca.transform(faces.data)\n",
    "    projected = pca.inverse_transform(components)\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    for i in range(10):\n",
    "        ax = fig.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
    "        ax.imshow(projected[10*i].reshape(faces.images[10*i].shape), cmap=plt.cm.bone)\n",
    "\n",
    "interact(dimensionality_reduction,\n",
    "         x=widgets.IntSlider(min=1, max=400, step=1, value=200,\n",
    "                             description='Number of Components:',\n",
    "                             style={'description_width':'initial'},\n",
    "                             layout=widgets.Layout(width='80%'),\n",
    "                             continuous_update=False));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion Question: <a id='4.1'></a>\n",
    "\n",
    "**What do you notice about the faces as you change the number of principle components? What do the faces look like with 1 component? What about with more components?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Replace this line with your answer. Double click this cell to begin typing.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of dimensionality reduction is in fact finding the principal components of the data. The first few principal components are the most important components in determining the underlying structure of the data. In terms of a dataset containing images of faces, the first few components are the most important in determining the overall appearance of the face. They are associated with the lighting conditions of the face, while later components deal with certain features like eyes, noses, lips, etc.\n",
    "\n",
    "Using this analysis, we can add and subtract different components until we get a good enough approximation of the data. By figuring out which components have the greatest importance, we are able to remove the less important components and in effect greatly reduce the size of the data we're working with while maintaining a close resemblance to the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Big Data Can Be Too Big! <a id='5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a href=\"https://www.pexels.com/photo/abstract-art-blur-bright-373543/\"><img src=\"images/blue-lights-big-data.jpg\" width=550 height=150></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Why Would We Simplify Data? <a id='5.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned before, one of the reasons we would perform dimensionality reduction on big data is to speed up computation. Keep in mind, when talking about an extremely large dataset, we're dealing with possibly billions to trillions of records of data. Any reduction or simplification of the data would positively affect processing and computing speed. Of course, it is also dependent on the machine doing the computation &mdash; a supercomputer will have much more computational power to work with big data.\n",
    "\n",
    "Another reason we would want to simplify data is to remove any noise from the dataset. Noise is essentially meaningless information contained within a dataset, such as corrupted or distorted data, and it often arrives through the data collection process. Ideally, we would want to remove any noisy data as it can negatively affect the results of any analysis.\n",
    "\n",
    "Lastly, simplifying data allows us to make any analysis more likely to be understandable to humans. Often when working with complex models and large datasets, it can become confusing to understand how these models or processes work and also what the data looks like, and these make it hard to know what kinds of inferences and conclusions we can draw. Especially in the context of machine learning, users that want to perform an analysis on big data with tools driven by machine learning might have a hard time understanding how a model works, and because of that have a hard time controlling it and drawing any meaningful results. Of course, users want to be able to, and simplifying the data along with simplifying how to interact with the model helps users with this goal.\n",
    "\n",
    "Abstracting away the low-level details of the user interface, or UI, of a model, is treating the model as a \"black box.\" Essentially, to simplify the data and the process, you create a mental model in which various inputs are put into the black box, which you assume works as it should, and the black box spits out various outputs. With this mental model, users only have to concern themselves with what they are inputting into the black box, and they don't have to understand how exactly the machine learning process works. If you're interested to learn more, check out this related [article](https://www.nngroup.com/articles/machine-learning-ux/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 What Are The Risks With Simplifying Data? <a id='5.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a href=\"https://pixabay.com/photos/head-skull-blow-resolution-resolve-2709732/\"><img src=\"images/bubble-head.png\" width=700 height=150></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the process of simplifying big data is not without its own risks. When we alter any data into something other than its original form, we run the risk of affecting our analysis. One risk data scientists encounter is when they change the data, they can inadvertently introduce bias into the dataset. This is because during the process of simplifying data, they make certain assumptions on the dataset in deciding to include, remove, combine, or alter certain data. Depending on these assumptions, the analysis can be greatly affected, and compromised or even incorrect conclusions can be drawn by the data scientists.\n",
    "\n",
    "Another potential risk is the risk of missing \"edge cases\" and its real-world implications. That is, if we simplify datasets to account for \"most\" variation, algorithms trained on that data can then fail to work well for edge cases. In real life, \"edge cases\" are often underrepresented minority groups in data. As previously mentioned, an example of this is facial recognition technology and the controversies with Google Images tagging dark-skinned people as “gorillas” and HP cameras only recognizing light-skinned faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion <a id='6'></a>\n",
    "\n",
    "In this lab you learned about _Big Data_ and how prevalent it is in our daily lives. You saw how _dimensions_ or features are used to measure the size of datasets, and how the size affects facial recognition in the interactive example. Then you explored the steps of computer facial recognition and the ways that humans reduce the dimensions of big data during analysis. You also learned about ethical limitations and consequences that can result from facial recognition as well as from simplifying data. These lessons and ideas can be applied to your later Cognitive Science coursework, and you can click on the links below to learn even more about these topics. \n",
    "\n",
    "Please fill out this [form](https://forms.gle/3D8foLB2JSJgKTdU8) to give us valuable feedback for later notebooks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Data Ethics Articles\n",
    "\n",
    "- [Amazon Employees Protest Sale of Facial Recognition Software](https://www.theverge.com/2018/6/22/17492106/amazon-ice-facial-recognition-internal-letter-protest)\n",
    "- [Facial Recognition Tech Is Growing Stronger, Thanks to Your Face](https://www.nytimes.com/2019/07/13/technology/databases-faces-facial-recognition-technology.html)\n",
    "- [How Facial Recognition Will Change Society](https://medium.com/@markvanrijmenam/seen-and-be-seen-how-facial-recognition-will-change-society-99dd8a8ed93a)\n",
    "- [Neurotech and Reforming Ethics](https://medium.com/@neurogress/how-neurotech-principles-are-changing-philosophy-and-reforming-ethics-60d0aeba9c9b)\n",
    "- [Racial Bias in Risk Assessment Algorithms](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)\n",
    "- [Who is Held Accountable When Facial Recognition Algorithms Fail?](https://medium.com/@ellenbroad/who-do-we-hold-accountable-when-facial-recognition-algorithms-fail-47feba3aae92)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography \n",
    "\n",
    "- Gael Varoquaux - Adapted example for \"Dimensionality Reduction\" section. https://scipy-lectures.org/packages/scikit-learn/auto_examples/plot_eigenfaces.html\n",
    "- Jake VanderPlas - Adapted example for \"Dimensionality Reduction\" section. https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.09-Principal-Component-Analysis.ipynb\n",
    "- Jupyter Widgets - Adapted code for widget. https://ipywidgets.readthedocs.io/en/stable/index.html\n",
    "- Raluca Budiu - Adapted article for \"Why Would We Simplify Data?\" section. https://www.nngroup.com/articles/machine-learning-ux/\n",
    "- Scikit-learn - Used Olivetti Faces dataset. https://scikit-learn.org/0.19/datasets/olivetti_faces.html\n",
    "- Viatcheslav Wlassoff - Adapted article for \"How Do Humans Recognize Faces?\" section. https://www.brainblogger.com/2015/10/17/how-the-brain-recognizes-faces/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Notebook developed by: Maria Sooklaris, Joshua Asuncion\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n",
    "\n",
    "Data Science Offerings at Berkeley: https://data.berkeley.edu/academics/undergraduate-programs/data-science-offerings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
